import { Bytes, BytesBlob } from "@typeberry/bytes";
import { FixedSizeArray } from "@typeberry/collections";
import type { ChainSpec } from "@typeberry/config";
import { check } from "@typeberry/utils";
import { ShardsCollection, decode, encode } from "reed-solomon-wasm/pkg";

/**
 * Shard size must be multiple of 64 bytes.
 * (reed-solomon-simd limitation: https://github.com/ordian/reed-solomon-simd)
 */
const SHARD_ALIGNMENT = 64;

/**
 * Number of main shards generated by EC. Togethe with redundancy shards they
 * create `N_SHARDS_TOTAL`.
 *
 * Any combination of this number of shards generated by EC is sufficient to
 * recover the original datum.
 *
 * https://graypaper.fluffylabs.dev/#/579bd12/3c55003c5500
 */
export const N_SHARDS_REQUIRED = 342;
export type N_SHARDS_REQUIRED = typeof N_SHARDS_REQUIRED;

/** Number of redundancy shards generated by EC. */
export const N_SHARDS_REDUNDANCY = 681;
/** Total number of shards generated by EC. */
export const N_SHARDS_TOTAL = 1023;
export type N_SHARDS_TOTAL = typeof N_SHARDS_TOTAL;
check(N_SHARDS_TOTAL === N_SHARDS_REQUIRED + N_SHARDS_REDUNDANCY);

/**
 * reed-solomon-simd requires shard size to be multiple of 64 bytes but we need only 2 bytes.
 * It does not matter what indices are selected, but it has to be n and n + 32
 */
const POINT_SIZE = 32;

/**
 * The shards are 2 bytes length because the encoding function is defined in GF(16)
 * https://graypaper.fluffylabs.dev/#/579bd12/3c17003c1700
 */
export const SHARD_LENGTH = 2;
export type SHARD_LENGTH = typeof SHARD_LENGTH;

export const CHUNK_SIZE = 684;
export type CHUNK_SIZE = typeof CHUNK_SIZE;
check(CHUNK_SIZE === N_SHARDS_REQUIRED * SHARD_LENGTH);

/**
 * Takes arbitrarily long input data, padds it to multiple of `CHUNK_SIZE` and returns
 * exactly `N_SHARDS_TOTAL` ec-coded segments.
 */
export function padAndEncodeData(input: BytesBlob) {
  // since we can only work with data that is a multiple of `CHUNK_SIZE` we pad it before
  // passing to `chunkingFunction`.
  const paddedLength = Math.ceil(input.length / CHUNK_SIZE) * CHUNK_SIZE;
  let padded = input;
  if (input.length !== paddedLength) {
    padded = BytesBlob.blobFrom(new Uint8Array(paddedLength));
    padded.raw.set(input.raw, 0);
  }
  return chunkingFunction(input);
}

/**
 * Decode possibly multi-chunk data from given array of required segments.
 *
 * NOTE the segment size MUST be a multiple of `SHARD_LENGTH` and each segment must
 * be the same length.
 *
 * NOTE the output data may have been padded to match the required size.
 */
export function decodeData(input: FixedSizeArray<[number, BytesBlob], N_SHARDS_REQUIRED>): BytesBlob {
  const n = input[0][1].length;
  const pieces = Math.floor(n / SHARD_LENGTH);
  check(pieces * SHARD_LENGTH === n);
  check(input.every(([_idx, piece]) => piece.length === n));

  const chunks = FixedSizeArray.fill(() => Bytes.zero(CHUNK_SIZE), pieces);

  for (let i = 0; i < pieces; i++) {
    const start = i * SHARD_LENGTH;
    const chunkShards = input.map<[number, Bytes<SHARD_LENGTH>]>(([index, piece]) => [
      index,
      Bytes.fromBlob(piece.raw.subarray(start, start + SHARD_LENGTH), SHARD_LENGTH),
    ]);

    chunks[i] = decodeChunk(FixedSizeArray.new(chunkShards, N_SHARDS_REQUIRED));
  }
  const laced = lace(chunks);
  return laced;
}

/**
 *  Erasure-encoding function. Takes exactly `CHUNK_SIZE` data and generate
 *
 *  https://graypaper.fluffylabs.dev/#/9a08063/3e4e013e5a01?v=0.6.6
 */
export function encodeChunk(input: Bytes<CHUNK_SIZE>): FixedSizeArray<Bytes<SHARD_LENGTH>, N_SHARDS_TOTAL> {
  const result = new Array<Bytes<SHARD_LENGTH>>();
  const data = new Uint8Array(SHARD_ALIGNMENT * N_SHARDS_REQUIRED);

  // add original shards to the result
  for (let i = 0; i < N_SHARDS_REQUIRED; i++) {
    const shardStart = SHARD_LENGTH * i;
    result.push(Bytes.fromBlob(input.raw.subarray(shardStart, shardStart + SHARD_LENGTH), SHARD_LENGTH));
    // fill array that will be passed to wasm lib
    for (let j = 0; j < SHARD_LENGTH; j++) {
      data[i * SHARD_ALIGNMENT + j * POINT_SIZE] = input.raw[shardStart + j];
    }
  }

  // encode and add redundancy shards
  const shards = new ShardsCollection(SHARD_ALIGNMENT, data);
  const encodedResult = encode(N_SHARDS_REDUNDANCY, SHARD_ALIGNMENT, shards);
  const encodedData = encodedResult.take_data();

  for (let i = 0; i < N_SHARDS_REDUNDANCY; i++) {
    const shardIdx = i * SHARD_ALIGNMENT;

    const redundancyShard = new Uint8Array(SHARD_LENGTH);
    for (let j = 0; j < SHARD_LENGTH; j++) {
      redundancyShard[j] = encodedData[shardIdx + j * POINT_SIZE];
    }
    result.push(Bytes.fromBlob(redundancyShard, SHARD_LENGTH));
  }

  return FixedSizeArray.new(result, N_SHARDS_TOTAL);
}

/**
 * The function takes exactly `N_SHARDS_TOTAL` shards to recover the original chunk.
 *
 * NOTE the chunk may contain padding, so it should be trimmed externally.
 */
export function decodeChunk(
  input: FixedSizeArray<[number, Bytes<SHARD_LENGTH>], N_SHARDS_REQUIRED>,
): Bytes<CHUNK_SIZE> {
  const result = Bytes.zero(CHUNK_SIZE);

  const data = new Uint8Array(N_SHARDS_REQUIRED * SHARD_ALIGNMENT);
  const indices = new Uint16Array(input.length);

  for (let i = 0; i < N_SHARDS_REQUIRED; i++) {
    const [index, points] = input[i];
    const shardStart = i * SHARD_ALIGNMENT;
    for (let j = 0; j < SHARD_LENGTH; j++) {
      data[shardStart + j * POINT_SIZE] = points.raw[j];
    }
    indices[i] = index;
    if (index < N_SHARDS_REQUIRED) {
      // fill original shards in result
      const shardStartInResult = SHARD_LENGTH * index;
      result.raw.set(points.raw, shardStartInResult);
    }
  }
  const shards = new ShardsCollection(SHARD_ALIGNMENT, data, indices);

  const decodingResult = decode(N_SHARDS_REQUIRED, N_SHARDS_REDUNDANCY, SHARD_ALIGNMENT, shards);
  const resultIndices = decodingResult.take_indices(); // it has to be called before take_data
  const resultData = decodingResult.take_data(); // it destroys the result object in rust

  if (resultIndices === undefined) {
    throw new Error("indices array in decoded result must exist!");
  }

  check(resultData.length === resultIndices.length * SHARD_ALIGNMENT, "incorrect length of data or indices!");

  for (let i = 0; i < resultIndices.length; i++) {
    // fill reconstructed shards in result
    const index = resultIndices[i];
    const resultIdx = SHARD_LENGTH * index;
    const shardIdx = i * SHARD_ALIGNMENT;
    for (let j = 0; j < SHARD_LENGTH; j++) {
      result.raw[resultIdx + j] = resultData[shardIdx + j * POINT_SIZE];
    }
  }

  return result;
}

/**
 * `split`: Takes a single BytesBlob and divides it into sequential,
 * contiguous chunks of a given size.
 *
 * Opposite of `join`.
 *
 * Input: [a0, a1, a2, a3, a4, a5], size = 2
 *
 * Output: [[a0, a1], [a2, a3], [a4, a5]]
 *
 * https://graypaper.fluffylabs.dev/#/9a08063/3eb4013eb401?v=0.6.6
 */
export function split<N extends number, K extends number>(input: BytesBlob, n: N, k: K): FixedSizeArray<Bytes<N>, K> {
  check(n * k === input.length);
  const result = new Array<Bytes<N>>();
  for (let i = 0; i < k; i++) {
    const start = i * n;
    const chunk = Bytes.zero(n);
    chunk.raw.set(input.raw.subarray(start, start + n));
    result[i] = chunk;
  }
  return FixedSizeArray.new(result, k);
}

/**
 * `join`: Takes an array of blobs and concatenates them sequentially
 * (one after another) into a single blob.
 *
 * The resulting blob has length of `N * K`.
 *
 * Opposite of `split`.
 *
 * Input: [[a0, a1], [a2, a3], [a4, a5]], size = 2
 *
 * Output: [a0, a1, a2, a3, a4, a5]
 *
 * https://graypaper.fluffylabs.dev/#/9a08063/3ed4013ed401?v=0.6.6
 */

export function join<N extends number, K extends number>(input: FixedSizeArray<Bytes<N>, K>): BytesBlob {
  return BytesBlob.blobFromParts(input.map((x) => x.raw));
}

/**
 * `unzip`: Reorganizes the data by de-interleaving: it takes every N-th byte
 * for each output chunk, where N is the number of output pieces.
 *
 * Opposite of `lace`.
 *
 * Input: [a0, b0, c0, a1, b1, c1], size = 2
 * Output: [[a0, a1], [b0, b1], [c0, c1]]
 *
 * https://graypaper.fluffylabs.dev/#/9a08063/3e06023e0602?v=0.6.6
 */
export function unzip<N extends number, K extends number>(input: BytesBlob, n: N, k: K): FixedSizeArray<Bytes<N>, K> {
  const result = Array.from({ length: k }, () => Bytes.zero(n));
  for (let i = 0; i < k; i++) {
    const entry = result[i].raw;
    for (let j = 0; j < n; j++) {
      entry[j] = input.raw[j * k + i];
    }
  }
  return FixedSizeArray.new(result, k);
}

/**
 * `lace`: Takes an array of blobs and interleaves their bytes:
 * it takes the first byte from each blob, then the second byte from
 * each blob, etc., producing a single blob.
 *
 * Opposite of `unzip`.
 *
 * Input: [[a0, a1], [b0, b1], [c0, c1]]
 *
 * Output: [a0, b0, c0, a1, b1, c1]
 *
 * https://graypaper.fluffylabs.dev/#/9a08063/3e2a023e2a02?v=0.6.6
 */
export function lace<N extends number, K extends number>(input: FixedSizeArray<Bytes<N>, K>): BytesBlob {
  const k = input.length;
  if (k === 0) {
    return BytesBlob.empty();
  }
  const n = input[0].length;
  const result = BytesBlob.blobFrom(new Uint8Array(k * n));
  for (let i = 0; i < k; i++) {
    const entry = input[i].raw;
    for (let j = 0; j < n; j++) {
      result.raw[j * k + i] = entry[j];
    }
  }
  return result;
}

/**
 * `T`: Transposing function which accepts an array of `k` pieces of data
 * which each have same lenght of `n` octets and returns an array of `n`
 * pieces of data which each have length of `k` octets.
 *
 * T[[x0,0, x0,1, x0,2, . . . ], [x1,0, x1,1, . . . ], . . . ] â‰¡
 * [[x0,0, x1,0, x2,0, . . . ], [x0,1, x1,1, . . . ], . . . ]
 *
 * https://graypaper.fluffylabs.dev/#/9a08063/3e2e023e2e02?v=0.6.6
 */
export function transpose<T, N extends number, K extends number>(
  input: FixedSizeArray<FixedSizeArray<T, K>, N>,
  k: K,
): FixedSizeArray<FixedSizeArray<T, N>, K> {
  const n = input.fixedLength;
  const columns = new Array<FixedSizeArray<T, N>>();

  for (let c = 0; c < k; c++) {
    const newColumn = new Array<T>();
    for (let r = 0; r < n; r++) {
      const cell = input[r][c];
      newColumn.push(cell);
    }
    columns.push(FixedSizeArray.new(newColumn, n));
  }
  return FixedSizeArray.new(columns, k);
}

/**
 * `C`: Erasure-code chunking function which accepts an arbitrary sized data blob whose
 *       length divides wholly into 684 octets and results in 1,023 sequences of sequences
 *       each of smaller blobs.
 *
 *       Each element of resulting array is the same length.
 *
 * https://graypaper.fluffylabs.dev/#/9a08063/3f15003f1500?v=0.6.6
 */
export function chunkingFunction(input: BytesBlob): FixedSizeArray<BytesBlob, N_SHARDS_TOTAL> {
  const k = Math.floor(input.length / CHUNK_SIZE);
  check(k * CHUNK_SIZE === input.length);

  // we get a `k` chunks.
  const unzipped = unzip<CHUNK_SIZE, typeof k>(input, CHUNK_SIZE, k);
  // and each chunk get's ec-codec
  const ecCodecChunks = unzipped.map((p) => encodeChunk(p));
  // hence we end up with a matrix of `EC_SHARDS` x `k`
  type EC_SHARDS = FixedSizeArray<Bytes<2>, N_SHARDS_TOTAL>;
  const chunksTyped: FixedSizeArray<EC_SHARDS, typeof k> = FixedSizeArray.new(ecCodecChunks, k);
  // next we transpose the array, getting back an array of `N_SHARDS_TOTAL` elements,
  // where each element is a `k` shards (`Bytes<2>`).
  const transposed = transpose(chunksTyped, N_SHARDS_TOTAL);
  // lastly we join each element of that resulting array
  // we get an array of `N_SHARDS_TOTAL` elements, each of length `SHARD_LENGTH * k`.
  const joined = transposed.map((c) => join(c));
  return FixedSizeArray.new(joined, N_SHARDS_TOTAL);
}

export function expandShardsToFullSet(chainSpec: ChainSpec, shards: BytesBlob[]): BytesBlob[] {
  if (shards.length === 1023) {
    // full set of shards, no need to expand
    return shards;
  }
  const shardSplitFactor = chainSpec.numberECPiecesPerSegment / 6;
  const shardLength = shards[0].length / shardSplitFactor;
  const result: BytesBlob[] = Array.from({ length: 1023 }, () => BytesBlob.blobFrom(new Uint8Array(shardLength)));
  for (let i = 0; i < shards.length; i++) {
    const shard = shards[i];
    let offset = 0;
    for (let j = 0; j < shard.length / 2; j += shardSplitFactor) {
      for (let k = 0; k < shardSplitFactor; k++) {
        const index = i * shardSplitFactor + k;
        if (index >= 1023) {
          continue; // Prevent overflow
        }
        const shardStart = (j + k) * 2;
        const shardEnd = shardStart + 2;
        result[index].raw.set(shard.raw.subarray(shardStart, shardEnd), offset);
      }
      offset += 2;
    }
  }

  return result;
}

export function condenseShardsFromFullSet(chainSpec: ChainSpec, shards: BytesBlob[]) {
  if (chainSpec.validatorsCount === 1023) {
    // full set of shards, no need to shrink
    return shards;
  }

  const shardSplitFactor = chainSpec.numberECPiecesPerSegment / 6;
  const condensedShardLength = shards[0].length * shardSplitFactor;
  const result: BytesBlob[] = Array.from({ length: chainSpec.validatorsCount }, () =>
    BytesBlob.blobFrom(new Uint8Array(condensedShardLength)),
  );
  for (let i = 0; i < chainSpec.validatorsCount; i++) {
    const shardToFill = result[i];
    for (let j = 0; j < shards[0].length; j++) {
      const shardIndex = j * shardSplitFactor;
      const offset = j * 2;
      for (let k = 0; k < shardSplitFactor; k++) {
        const sourceIndex = i * shardSplitFactor + k;
        if (sourceIndex >= shards.length) {
          continue; // Prevent overflow
        }
        const chunkToCopy = shards[sourceIndex].raw.subarray(offset, offset + 2);
        const destinationOffset = (shardIndex + k) * 2;
        if (destinationOffset + chunkToCopy.length > shardToFill.raw.length) {
          continue; // Prevent overflow
        }
        shardToFill.raw.set(chunkToCopy, destinationOffset);
      }
    }
  }

  return result;
}
